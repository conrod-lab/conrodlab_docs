<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Datasets &mdash; Courtois NeuroMod 2020-beta documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="shortcut icon" href="_static/logo_neuromod_small.png"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Access" href="ACCESS.html" />
    <link rel="prev" title="Courtois NeuroMod" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Courtois NeuroMod
            <img src="_static/logo_neuromod_black.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Datasets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#bids">BIDS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#participants">Participants</a></li>
<li class="toctree-l2"><a class="reference internal" href="#anat">anat</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hcptrt">hcptrt</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#gambling">Gambling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#motor">Motor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#language-processing">Language processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#social-cognition">Social cognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#relational-processing">Relational processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#emotion-processing">Emotion processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#working-memory">Working memory</a></li>
<li class="toctree-l3"><a class="reference internal" href="#resting-state">Resting state</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#movie10">movie10</a></li>
<li class="toctree-l2"><a class="reference internal" href="#friends">Friends</a></li>
<li class="toctree-l2"><a class="reference internal" href="#harrypotter">harrypotter</a></li>
<li class="toctree-l2"><a class="reference internal" href="#shinobi-training">shinobi_training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#shinobi">shinobi</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ACCESS.html">Access</a></li>
<li class="toctree-l1"><a class="reference internal" href="MRI.html">MRI</a></li>
<li class="toctree-l1"><a class="reference internal" href="DERIVATIVES.html">Derivatives</a></li>
<li class="toctree-l1"><a class="reference internal" href="RELEASES.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="COC.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="AUTHORS.html">Authors</a></li>
<li class="toctree-l1"><a class="reference internal" href="ACKNOWLEDGMENT.html">How to acknowledge</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Courtois NeuroMod</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">Datasets</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/DATASETS.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="datasets">
<h1>Datasets<a class="headerlink" href="#datasets" title="Permalink to this heading"></a></h1>
<section id="bids">
<h2>BIDS<a class="headerlink" href="#bids" title="Permalink to this heading"></a></h2>
<p>All functional and anatomical data has been formatted in <a class="reference external" href="https://bids.neuroimaging.io/">BIDS</a>, for more information visit the Brain Imaging Data Structure documentation <a class="reference external" href="https://bids-specification.readthedocs.io/en/stable/">site</a>.
Some of the files do not follow the main BIDS convention:</p>
<ul class="simple">
<li><p>Anatomical sequences with multiple contrasts are following <a class="reference external" href="https://bids.neuroimaging.io/bep001">BEP001</a>.</p></li>
<li><p>Spinal cord imaging use Body Part tag proposed in <a class="reference external" href="https://bids.neuroimaging.io/bep025">BEP025</a> (<code class="docutils literal notranslate"><span class="pre">bp-cspine</span></code>) to allow to distinguish them from brain anatomical images acquired with the same contrasts.</p></li>
</ul>
<p>Note that BIDS session names have no meaning apart from being data acquired in the same session. The number of runs, the tasks and their order within each session will not match from one participant to another. Note that a few session indices are skipped if the whole session was discarded for various scanning issues.</p>
</section>
<section id="participants">
<h2>Participants<a class="headerlink" href="#participants" title="Permalink to this heading"></a></h2>
<p>Six healthy participants (aged 31 to 47 at the time of recruitment in 2018), 3 women (<code class="docutils literal notranslate"><span class="pre">sub-03</span></code>, <code class="docutils literal notranslate"><span class="pre">sub-04</span></code> and <code class="docutils literal notranslate"><span class="pre">sub-06</span></code>) and 3 men (<code class="docutils literal notranslate"><span class="pre">sub-01</span></code>, <code class="docutils literal notranslate"><span class="pre">sub-02</span></code> and <code class="docutils literal notranslate"><span class="pre">sub-05</span></code>) consented to participate in the Courtois Neuromod Project for at least 5 years. Three of the participants reported being native francophone speakers (<code class="docutils literal notranslate"><span class="pre">sub-01</span></code>, <code class="docutils literal notranslate"><span class="pre">sub-02</span></code> and <code class="docutils literal notranslate"><span class="pre">sub-04</span></code>), one as being a native anglophone (<code class="docutils literal notranslate"><span class="pre">sub-06</span></code>) and two as bilingual native speakers (<code class="docutils literal notranslate"><span class="pre">sub-03</span></code> and <code class="docutils literal notranslate"><span class="pre">sub-05</span></code>).   All participants reported the right hand as being their dominant hand and reported being in good general health.</p>
<p>Exclusion criteria included visual or auditory impairments that would prevent participants from seeing and/or hearing stimuli in the scanner and major psychiatric or neurological problems. Standard exclusion criteria for MRI and MEG were also applied. Lastly, given that all stimuli and instructions are presented in English, all participants had to report having an advanced comprehension of the English language for inclusion.</p>
</section>
<section id="anat">
<h2>anat<a class="headerlink" href="#anat" title="Permalink to this heading"></a></h2>
<p>The anatomical dataset includes longitudinal anatomical images of the brain and upper spinal cord at an approximate rate of 4 sessions a year. The primary intended use of this dataset is to monitor the structural stability of the brain of participants for the duration of the study. Many quantitative measures of brain structure can also be derived and included in analyses, such as gray matter morphometry, tractography or measures of myelination.</p>
<p>The MRI sequences are described in more detailed in <a class="reference internal" href="MRI.html#brain-anatomical-sequences"><span class="std std-ref">Brain anatomical sequences</span></a> and <a class="reference internal" href="MRI.html#spinal-cord-anatomical-sequences"><span class="std std-ref">Spinal cord anatomical sequences</span></a>, including pdfs of the Siemens exam cards.</p>
<p>Brain T1w, T2w and DWI were copied from the HCP aging and development protocol for Prisma MRI scanner.
Other sequences were selected and optimized by the Courtois NeuroMod team.</p>
<p>All images covering the face were anonymized by zeroing the data in the face, teeth and ears regions with a custom mask warped from the MNI space based on a linear registration of the T1w brain MRI series. This defacing script is available <a class="reference external" href="https://github.com/courtois-neuromod/ds_prep/blob/main/mri/prepare/deface_anat.py">here</a></p>
</section>
<section id="hcptrt">
<h2>hcptrt<a class="headerlink" href="#hcptrt" title="Permalink to this heading"></a></h2>
<p>This <code class="docutils literal notranslate"><span class="pre">cneuromod</span></code> dataset is called HCP test-retest (<code class="docutils literal notranslate"><span class="pre">hcptrt</span></code>), because participants repeated 15 times the functional localizers developed by the Human Connectome Project, for a total of approximately 10 hours of functional data per subject. The protocol consisted of seven tasks, described below (text adapted from the <a class="reference external" href="http://protocols.humanconnectome.org/HCP/3T/task-fMRI-protocol-details.html">HCP protocol</a>). Before each task, participants were given detailed instructions and examples, as well as a practice run. A session was typically composed either of two repetitions of the HCP localizers, or one resting-state run and one HCP localizer. The eprime scripts for preparation and presentation of the stimuli can be found in the <a class="reference external" href="https://db.humanconnectome.org/app/action/ChooseDownloadResources?project=HCP_Resources&amp;resource=Scripts&amp;filePath=HCP_TFMRI_scripts.zip">HCP database</a>. Stimuli and e-prime scripst were provided by the Human Connectome Project, U-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research, and by the McDonnell Center for Systems Neuroscience at Washington University. Note that in the <code class="docutils literal notranslate"><span class="pre">cneuromod</span></code> DataLad, functional runs are named <code class="docutils literal notranslate"><span class="pre">func_sub-&lt;participant&gt;_ses-&lt;sess&gt;_task-&lt;task&gt;_run-&lt;run&gt;</span></code>, where the <code class="docutils literal notranslate"><span class="pre">&lt;participant&gt;</span></code> tag includes <code class="docutils literal notranslate"><span class="pre">sub-01</span></code> to <code class="docutils literal notranslate"><span class="pre">sub-06</span></code>. For each functional run, a companion file <code class="docutils literal notranslate"><span class="pre">_events.tsv</span></code> contains the timing and type of events presented to the subject. Session tags <code class="docutils literal notranslate"><span class="pre">&lt;sess&gt;</span></code> are <code class="docutils literal notranslate"><span class="pre">001</span></code>, <code class="docutils literal notranslate"><span class="pre">002</span></code> etc, and the number and composition of sessions vary from subject to subject. The <code class="docutils literal notranslate"><span class="pre">&lt;task&gt;</span></code> tags are <code class="docutils literal notranslate"><span class="pre">restingstate</span></code>, <code class="docutils literal notranslate"><span class="pre">gambling</span></code>, <code class="docutils literal notranslate"><span class="pre">motor</span></code>, <code class="docutils literal notranslate"><span class="pre">social</span></code>, <code class="docutils literal notranslate"><span class="pre">wm</span></code>, <code class="docutils literal notranslate"><span class="pre">emotion</span></code>, <code class="docutils literal notranslate"><span class="pre">language</span></code> and <code class="docutils literal notranslate"><span class="pre">relational</span></code>, as described below. Tasks that were repeated twice have separate <code class="docutils literal notranslate"><span class="pre">&lt;run&gt;</span></code> tags (<code class="docutils literal notranslate"><span class="pre">01</span></code>, <code class="docutils literal notranslate"><span class="pre">02</span></code>).</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The duration of BOLD series are slightly varying across participants and repetitions. If consistent length is required by analysis, series can be trimmed at the end to match duration, task being aligned to the first TR.</p>
</div>
<section id="gambling">
<h3><a class="reference external" href="http://www.cognitiveatlas.org/task/id/trm_550b5c1a7f4db/">Gambling</a><a class="headerlink" href="#gambling" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">gambling</span></code> duration: approximately 3 minutes. Participants were asked to guess whether a hidden number (represented by a “?” during 1500ms) was above or below 5 <a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2004.10.002">(Delgado et al. 2000)</a>. They indicated their choice using a button press, and were then shown the actual number. If they guessed correctly they were told they won money (+$1.00, <code class="docutils literal notranslate"><span class="pre">win</span></code> trial), if they guessed incorrectly they were told they lost money (-$0.50, <code class="docutils literal notranslate"><span class="pre">loss</span></code> trial), and if the number was exactly 5 they were told that they neither won or lost money ($0, <code class="docutils literal notranslate"><span class="pre">neutral</span></code> trial). Note that no money was actually given to the participants and, as such, this task may not be an accurate reproduction of the HCP protocol. The conditions were presented in blocks of 8 trials of type <code class="docutils literal notranslate"><span class="pre">reward</span></code> (6 <code class="docutils literal notranslate"><span class="pre">win</span></code> trials pseudo randomly interleaved with either 1 <code class="docutils literal notranslate"><span class="pre">neutral</span></code> and 1 <code class="docutils literal notranslate"><span class="pre">loss</span></code> trial, 2 <code class="docutils literal notranslate"><span class="pre">neutral</span></code> trials, or 2 <code class="docutils literal notranslate"><span class="pre">loss</span></code> trials) or of type <code class="docutils literal notranslate"><span class="pre">punishment</span></code> (6 <code class="docutils literal notranslate"><span class="pre">loss</span></code> trials pseudo-randomly interleaved with either 1 <code class="docutils literal notranslate"><span class="pre">neutral</span></code> and 1 <code class="docutils literal notranslate"><span class="pre">win</span></code> trial, 2 <code class="docutils literal notranslate"><span class="pre">neutral</span></code> trials, or 2 <code class="docutils literal notranslate"><span class="pre">win</span></code> trials). There were four blocks per run (2 <code class="docutils literal notranslate"><span class="pre">reward</span></code> and 2 <code class="docutils literal notranslate"><span class="pre">punishment</span></code>), and two runs in total.</p>
</section>
<section id="motor">
<h3><a class="reference external" href="http://www.cognitiveatlas.org/task/id/trm_550b53d7dd674/">Motor</a><a class="headerlink" href="#motor" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">motor</span></code> duration: approximately 3 minutes. This task was adapted from (Buckner et al. 2011; Yeo et al. 2011). Participants were presented a visual cue, and were asked to either tap their left or right fingers (event types <code class="docutils literal notranslate"><span class="pre">left_hand</span></code> and <code class="docutils literal notranslate"><span class="pre">right_hand</span></code>, resp.), squeeze their left or right toes (event types <code class="docutils literal notranslate"><span class="pre">left_foot</span></code> and <code class="docutils literal notranslate"><span class="pre">right_foot</span></code>, resp.), or move their tongue to map motor area (event type <code class="docutils literal notranslate"><span class="pre">tongue</span></code>). Each movement lasted 12 seconds, and in total there were 13 blocks, with 2 of <code class="docutils literal notranslate"><span class="pre">tongue</span></code> movements, 4 of hand movements (2 <code class="docutils literal notranslate"><span class="pre">right_hand</span></code> and 2 <code class="docutils literal notranslate"><span class="pre">left_hand</span></code>), and 4 of foot movements (2 <code class="docutils literal notranslate"><span class="pre">right_foot</span></code> and 2 <code class="docutils literal notranslate"><span class="pre">left_foot</span></code>), and three 15 second fixation blocks where participants were instructed not to move anything. There were two runs in total, and 13 blocks per run.</p>
</section>
<section id="language-processing">
<h3><a class="reference external" href="http://www.cognitiveatlas.org/task/id/trm_550b54a8b30f4/">Language processing</a><a class="headerlink" href="#language-processing" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">language</span></code> duration: approximately 4 minutes. Participants were presented with two types of events. During <code class="docutils literal notranslate"><span class="pre">story</span></code> events, participants listened to an auditory story (5-9 sentences, about 20 seconds), followed by a two-alternative forced-choice question. During <code class="docutils literal notranslate"><span class="pre">math</span></code> events, they listened to a math problem (addition and subtraction only, varies in length), and were instructed to push a button to select the first or the second answer as being correct. The task was adaptive so that for every correct answer the level of difficulty increased. The math task was designed this way to maintain the same level of difficulty between participants. There were 2 runs, each with 4 <code class="docutils literal notranslate"><span class="pre">story</span></code> and 4 <code class="docutils literal notranslate"><span class="pre">math</span></code> blocks, interleaved.</p>
</section>
<section id="social-cognition">
<h3><a class="reference external" href="http://www.cognitiveatlas.org/task/id/trm_550b557e5f90e/">Social cognition</a><a class="headerlink" href="#social-cognition" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">social</span></code> duration: approximately 3 minutes. Participants were presented with short video clips (20 seconds) of objects (squares, circles, triangles) that either interacted in some way (event type <code class="docutils literal notranslate"><span class="pre">mental</span></code>), or moved randomly on the screen (event type <code class="docutils literal notranslate"><span class="pre">random</span></code>) (<a class="reference external" href="https://doi.org/10.1006/nimg.2000.0612">Castelli et al. 2000</a>; <a class="reference external" href="https://doi.org/10.1111/j.1467-9280.2007.01923.x">Wheatley et al. 2007</a>). Following each clip, participants were asked to judge whether the objects had a “Mental interaction” (an interaction that appeared as if the shapes were taking into account each other’s feelings and thoughts), whether the were “Not Sure”, or if there was “No interaction”. Button presses were used to record their responses. In each of the two runs, participants viewed 5 <code class="docutils literal notranslate"><span class="pre">mental</span></code> videos and 5 <code class="docutils literal notranslate"><span class="pre">random</span></code> videos, and had 5 fixation blocks of 15 seconds each.</p>
</section>
<section id="relational-processing">
<h3><a class="reference external" href="http://www.cognitiveatlas.org/task/id/trm_550b5a47aa23e/">Relational processing</a><a class="headerlink" href="#relational-processing" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">relational</span></code> duration: approximately 3 minutes. Participants were shown 6 different shapes filled with 1 of 6 different textures (Smith et al. 2007). There were two conditions: relations processing (event type <code class="docutils literal notranslate"><span class="pre">relational</span></code>), and control matching condition (event type <code class="docutils literal notranslate"><span class="pre">control</span></code>). In the <code class="docutils literal notranslate"><span class="pre">relational</span></code> events, 2 pairs of objects were presented on the screen, with one pair at the top of the screen, and the other pair at the bottom. Participants were instructed to decide what dimension differed in the top pair (shape or texture), and then decide if the bottom pair differed, or not, on the same dimension (i.e. if the top pair differed in shape, did the bottom pair also differ in shape). Their answers were recorded by one of two button presses: “a” differ on same dimension; “b” don’t differ on same dimension. In the <code class="docutils literal notranslate"><span class="pre">control</span></code> events, participants were shown two objects at the top of the screen, and one object at the bottom of the screen, with a word in the middle of the screen (either “shape” or “texture”).They were told to decide whether the bottom object matched either of the top two objects on that dimension (i.e., if the word is “shape”, did the bottom object have the same shape as either of the top two objects). Participants responded “yes” or “no” using the button box. For the <code class="docutils literal notranslate"><span class="pre">relational</span></code> condition, the stimuli were presented for 3500 ms, with a 500 ms ITI, and there were four trials per block. In the <code class="docutils literal notranslate"><span class="pre">control</span></code>condition, stimuli were presented for 2800 ms, with a 400 ms ITI, and there were 5 trials per block. In total there were two runs, each with three <code class="docutils literal notranslate"><span class="pre">relational</span></code> blocks, three <code class="docutils literal notranslate"><span class="pre">control</span></code> blocks and three 16-second fixation blocks.</p>
</section>
<section id="emotion-processing">
<h3><a class="reference external" href="http://www.cognitiveatlas.org/task/id/trm_550b5b066d37b/">Emotion processing</a><a class="headerlink" href="#emotion-processing" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">emotion</span></code> duration: approximately 2 minutes. Participants were shown triads of faces (event type <code class="docutils literal notranslate"><span class="pre">face</span></code>) or shapes (event type <code class="docutils literal notranslate"><span class="pre">shape</span></code>), and were asked to decide which of the shapes at the bottom of the screen matches the target face/ shape at the top of the screen (adapted from Smith et al. 2007). Faces had either an angry or fearful expression. Faces, and shapes were presented in three blocks of 6 trials (3 <code class="docutils literal notranslate"><span class="pre">face</span></code> and 3 <code class="docutils literal notranslate"><span class="pre">shape</span></code>), with each trial lasting 2 seconds, followed by a 1 second inter-stimulus interval. Each block was preceded by a 3000 ms task cue (“shape” or “face”), so that each block was 21 seconds long, including the cue. In total there were two runs, three <code class="docutils literal notranslate"><span class="pre">face</span></code> blocks and three <code class="docutils literal notranslate"><span class="pre">shape</span></code> blocks, with 8 seconds of fixation at the end of each run.</p>
</section>
<section id="working-memory">
<h3><a class="reference external" href="http://www.cognitiveatlas.org/task/id/trm_550b50095d4a3/">Working memory</a><a class="headerlink" href="#working-memory" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">wm</span></code> duration: approximately 5 minutes. There were two subtasks: a category specific representation, and a working memory task. Participants were presented with blocks of either places, tools, faces, and body parts. Within each run, all 4 types of stimuli were presented in block, with each block being labelled as a 2-back task (participants needed to indicate if they saw the same image two images back), or a version of a 0-back task (participants were shown a target at the start of the trial and they needed to indicate if the image that they were seeing matched the target). There were thus 8 different event types <code class="docutils literal notranslate"><span class="pre">&lt;stim&gt;_&lt;back&gt;</span></code>, where <code class="docutils literal notranslate"><span class="pre">&lt;stim&gt;</span></code> was one of <code class="docutils literal notranslate"><span class="pre">place</span></code>, <code class="docutils literal notranslate"><span class="pre">tools</span></code>, <code class="docutils literal notranslate"><span class="pre">face</span></code> or <code class="docutils literal notranslate"><span class="pre">body</span></code>, and <code class="docutils literal notranslate"><span class="pre">&lt;back&gt;</span></code> was one of <code class="docutils literal notranslate"><span class="pre">0back</span></code> or <code class="docutils literal notranslate"><span class="pre">2back</span></code>. Each image was presented for 2 seconds, followed by a 500 ms ITI. Stimuli were presented for 2 seconds, followed by a 500 ms inter-task interval. Each of the 2 runs included 8 event types with 10 trials per type, as well as 4 fixations blocks (15 secs).</p>
</section>
<section id="resting-state">
<h3><a class="reference external" href="http://www.cognitiveatlas.org/task/id/trm_4c8a834779883/">Resting state</a><a class="headerlink" href="#resting-state" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">restingstate</span></code> duration: 15 minutes. In every other session, one resting-state fMRI run was acquired, giving 5 runs per participant. Participants were asked to have their eye open, be looking at fixation cross in the middle of the screen and be instructed to not fall asleep. A total of five resting-state fMRI runs were acquired per subject.</p>
</section>
</section>
<section id="movie10">
<h2>movie10<a class="headerlink" href="#movie10" title="Permalink to this heading"></a></h2>
<p>This dataset includes about 10 hours of functional data for all 6 participants. The python &amp; psychopy scripts for preparation and presentation of the clips can be found in <code class="docutils literal notranslate"><span class="pre">src/tasks/video.py</span></code> of the following github <a class="reference external" href="https://github.com/courtois-neuromod/task_stimuli">repository</a>.
Session tags <code class="docutils literal notranslate"><span class="pre">&lt;sess&gt;</span></code> were <code class="docutils literal notranslate"><span class="pre">001</span></code>, <code class="docutils literal notranslate"><span class="pre">002</span></code> etc, and the number and composition of sessions varied from subject to subject. The <code class="docutils literal notranslate"><span class="pre">&lt;task&gt;</span></code> tags used in DataLad corresponded to each movie (<code class="docutils literal notranslate"><span class="pre">bourne</span></code>, <code class="docutils literal notranslate"><span class="pre">wolf</span></code>, <code class="docutils literal notranslate"><span class="pre">life</span></code>, <code class="docutils literal notranslate"><span class="pre">figures</span></code>) and a numerical index of the segments shown as each movie was cut into roughly ten minutes segments presented in separate run. Exact cutting points were manually selected to not interrupt the narrative flow. Fade out to a black screen was added at the end of each clip, and with a few seconds overlap between the end of a clip and the beginning of the next clip. The movie segments can be found under <code class="docutils literal notranslate"><span class="pre">movie10/stimuli/&lt;movie&gt;/&lt;movie&gt;_seg&lt;seg&gt;.mkv</span></code>, and the functional runs are named <code class="docutils literal notranslate"><span class="pre">func_sub-&lt;participant&gt;_ses-&lt;sess&gt;_task-&lt;movie&gt;&lt;seg&gt;</span></code>, where the <code class="docutils literal notranslate"><span class="pre">&lt;participant&gt;</span></code> tag ranges from <code class="docutils literal notranslate"><span class="pre">sub-01</span></code> to <code class="docutils literal notranslate"><span class="pre">sub-06</span></code>. A companion file <code class="docutils literal notranslate"><span class="pre">_events.tsv</span></code> contains the timing and type of conditions presented to the subject.</p>
<p>The participants watched the following movies (<a class="reference external" href="https://www.cognitiveatlas.org/id/trm_4c898da401420/">cogatlas</a>):</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;task&gt;</span></code> name <code class="docutils literal notranslate"><span class="pre">bourne</span></code>: <a class="reference external" href="https://en.wikipedia.org/wiki/The_Bourne_Supremacy_%28film%29">The Bourne supremacy</a>. Duration ~100 minutes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;task&gt;</span></code> name <code class="docutils literal notranslate"><span class="pre">wolf</span></code>: <a class="reference external" href="https://en.wikipedia.org/wiki/The_Wolf_of_Wall_Street_%282013_film%29">The wolf of wall street</a>. Duration ~170 minutes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;task&gt;</span></code> name <code class="docutils literal notranslate"><span class="pre">figures</span></code>: <a class="reference external" href="https://en.wikipedia.org/wiki/Hidden_Figures">Hidden figures</a>. Duration ~120 minutes. This movie was presented twice, for a total duration of ~240 minutes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;task&gt;</span></code> name <code class="docutils literal notranslate"><span class="pre">life</span></code>: <a class="reference external" href="https://en.wikipedia.org/wiki/Life_(British_TV_series)">Life</a> Disc one of four: “Challenges of life, reptiles and amphibian mammals”. DVD set was narrated by David Attenborough. Duration, and lasted ~50 minutes. This movie was presented twice, for a total duration of ~100 minutes.</p></li>
</ul>
<p>It should be noted that although three of the participants are not native anglophones, all participants watched the movies in English. The three native francophone participants are fluent in English and report regularly watching movies in English.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The duration of BOLD series are slightly varying across participants and repetitions. If consistent length is required for analysis, series can be trimmed at the end to match duration, movie segments being aligned to the first TR.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>There are instances of re-scanned segments (due to scan QC fail), these re-scans will be in separate sessions. These should be handled or excluded in analysis requiring continuity of the presentation of the story.</p>
</div>
</section>
<section id="friends">
<h2>Friends<a class="headerlink" href="#friends" title="Permalink to this heading"></a></h2>
<p>This dataset contains functional data acquired while showing participants episodes of the Friends TV show in English. It includes seasons 1-6 for all subjects, except <code class="docutils literal notranslate"><span class="pre">sub-04</span></code> who only completed seasons 1-4 (and a few segments of season 5). Each episode is cut in two segments (a/b) to allow more flexible scanning and give participants opportunities for breaks. There is a small overlap between the segments to allow participants to catch up with the storyline. The task BIDS entity identifies the season, episode and segments (a/b) as such <code class="docutils literal notranslate"><span class="pre">task-s&lt;eason&gt;e&lt;pisode&gt;[ab]</span></code>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>A mistake happened when ripping the first season, causing <code class="docutils literal notranslate"><span class="pre">s01e01</span></code> and <code class="docutils literal notranslate"><span class="pre">s01e06</span></code> to be swapped in name and order of presentation. Files were renamed afterward to match external data such as annotations. However the order of presentation remains, slightly disrupting the storyline presented to the participant.</p>
</div>
</section>
<section id="harrypotter">
<h2>harrypotter<a class="headerlink" href="#harrypotter" title="Permalink to this heading"></a></h2>
<p>This dataset contains a single session per participant (N=5) when they read chapter 9 from chapter 9 of Harry Potter and the Sorcerer’s Stone. The text was presented word by word, at a 2Hz pace (each word presented for .5s). This chapter was split over 7 runs of approximate equal length. The stimuli used in this dataset are taken from the experiment reported by <a class="reference external" href="https://www.biorxiv.org/content/10.1101/2020.09.28.316935v4.full.pdf#cite.wehbe2014">Wehbe et al. (2014)</a> for which a separate fMRI dataset (N=9) has been collected and shared.</p>
</section>
<section id="shinobi-training">
<h2>shinobi_training<a class="headerlink" href="#shinobi-training" title="Permalink to this heading"></a></h2>
<p>This is a pure behavioral dataset collected while participants trained at home on the videogame Shinobi III The Return of the Ninja Master.
No training regimen was imposed to the participant making that dataset highly heterogeneous. It consists of sessions of gameplay as collections of bk2 files recorded by the <a class="reference external" href="https://github.com/openai/retro">gym-retro</a> API.
A subset of 3 levels of the game was selected for their difference in terms of game mechanics, requiring to acquire different skills in each.
This dataset can be used to analyze learning or individual game-play styles, and can be investigated in conjunction with the fMRI dataset.</p>
</section>
<section id="shinobi">
<h2>shinobi<a class="headerlink" href="#shinobi" title="Permalink to this heading"></a></h2>
<p>This dataset contains about 10h of gameplay on the videogame Shinobi III The Return of the Ninja Master, for N=4 participants (<code class="docutils literal notranslate"><span class="pre">sub-01</span></code>, <code class="docutils literal notranslate"><span class="pre">sub-02</span></code>, <code class="docutils literal notranslate"><span class="pre">sub-04</span></code> and <code class="docutils literal notranslate"><span class="pre">sub-06</span></code>). Participants used a custom-built fully fiber-optic MRI controller, designed by the team and described in <a class="reference external" href="https://psyarxiv.com/m2x6y/">Harel et al. (2022)</a>. In each run, participants played 3 levels in cycles and always in the same order. These levels were selected in the game to have fairly homogeneous game mechanics (see the <a class="reference external" href="https://sega.fandom.com/wiki/Shinobi_III:_Return_of_the_Ninja_Master">Sega documentation</a> for more details on game structure):</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Level-1</span></code> corresponded to round 1of the original game, “Zeed’s Resurrection”. It included one mini-boss and one boss fight.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Level-4</span></code> corresponded to the beginning of round 4 of the original game, “Destruction”. It included no mini-boss or boss fight.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Level-5</span></code> corresponded to the beginning of round 5 of the original game, “Electric demon”. It included one mini-boss fight and no boss fight.</p></li>
</ul>
<p>Participants moved to the next level if they successfully completed a level, or lost three lives. A new level was then initiated unless 10 minutes had elapsed from the start of the run, at which point the run ended. The duration of each run is thus variable to a degree, with a minimum of ten minutes. Due to the fixed order in the cycle, <code class="docutils literal notranslate"><span class="pre">Level-1</span></code> was repeated more often than <code class="docutils literal notranslate"><span class="pre">Level-2</span></code> and <code class="docutils literal notranslate"><span class="pre">Level-3</span></code>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Due to a programming error a certain number of game recording files were lost during acquisition, these repetitions are still listed in the events file but have a <code class="docutils literal notranslate"><span class="pre">stim_file</span></code> is left blank. Choice is left to the user whether to exclude the corresponding fMRI volumes or not for their analysis</p>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Courtois NeuroMod" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ACCESS.html" class="btn btn-neutral float-right" title="Access" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Courtois NeuroMod team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>